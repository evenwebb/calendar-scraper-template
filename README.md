<div align="center">

# ğŸ“… Calendar Scraper Template

A reusable Python template for building event scrapers that generate iCalendar (`.ics`) feeds and static HTML calendar pages.

</div>

---

## ğŸ“š Table of Contents

- [âš¡ Quick Start](#-quick-start)
- [âœ¨ Features](#-features)
- [ğŸ“¦ Installation](#-installation)
- [ğŸš€ Usage](#-usage)
- [âš™ï¸ Configuration](#ï¸-configuration)
- [ğŸ¤– GitHub Actions Automation](#-github-actions-automation)
- [ğŸ—‚ï¸ Project Structure](#ï¸-project-structure)
- [ğŸ§© Dependencies](#-dependencies)
- [ğŸ› ï¸ Troubleshooting](#ï¸-troubleshooting)
- [ğŸ“„ License](#-license)

---

## âš¡ Quick Start

```bash
git clone https://github.com/evenwebb/calendar-scraper-template.git
cd calendar-scraper-template
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
python3 init_scraper.py
python3 scraper.py
```

âœ… Generated outputs are written to `docs/` by default.

---

## âœ¨ Features

| Feature | Description |
|---|---|
| `ğŸ”„ Multiple Extraction Methods` | JSON, HTML, text parsing, or REST API adapters depending on your source. |
| `ğŸ’¾ Smart Caching` | Event detail cache and state tracking reduce unnecessary re-scraping and writes. |
| `âš¡ Optimized Updates` | Supports skipping full updates when no new upcoming events are detected. |
| `ğŸ“± HTML + iCal Output` | Generates both `.ics` feeds and static `index.html`/`archive.html` pages. |
| `ğŸ”” Notification Support` | Optional VALARM notification blocks for event reminders in calendar clients. |
| `ğŸŒ Timezone Aware` | Handles timezone-aware event serialization with configurable defaults. |
| `ğŸ¤– Automation Ready` | Includes GitHub Actions workflow with retries and optional failure issue creation. |

---

## ğŸ“¦ Installation

```bash
git clone https://github.com/evenwebb/calendar-scraper-template.git
cd calendar-scraper-template
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

Optional extras (enable in `requirements.txt` if needed):

- `beautifulsoup4` for HTML extraction
- `python-dotenv` for `.env` loading

---

## ğŸš€ Usage

```bash
python3 scraper.py
```

Typical output files:

- `docs/<ICS_FILENAME>.ics`
- `docs/index.html`
- `docs/archive.html`

---

## âš™ï¸ Configuration

Primary configuration lives in `config.py` (environment variables can override key values).

| Option | Default | Description |
|---|---|---|
| `EVENTS_URL` | `https://example.com/events` | Source events URL. |
| `BASE_URL` | `https://example.com` | Base URL for absolute links. |
| `OUTPUT_DIR` | `docs` | Output directory for generated files. |
| `ICS_FILENAME` | `calendar` | Output `.ics` base filename. |
| `EXTRACTION_METHOD` | `json` | One of `json`, `html`, `text`, `api`. |
| `HTTP_TIMEOUT` | `60` | HTTP timeout in seconds. |
| `HTTP_RETRIES` | `3` | Number of HTTP retries per request. |
| `CACHE_FILE` | `.event_cache.json` | Event detail cache file. |
| `CACHE_EXPIRY_DAYS` | `7` | Cache retention period. |
| `SKIP_IF_NO_NEW_EVENTS` | `True` | Skip full scrape when no new events are detected. |
| `INCLUDE_PAST_EVENTS` | `True` | Include past events in output. |
| `MAX_EVENTS` | `0` | Event cap (`0` = unlimited). |
| `DEFAULT_TIMEZONE` | `UTC` | Fallback timezone for events. |

### ğŸ§± Template placeholders to replace before production

- `CALENDAR_NAME`
- `CALENDAR_DESCRIPTION`
- `CALENDAR_PRODID`
- `UID_DOMAIN`
- `SITE_NAME`
- `SITE_TAGLINE`
- `SITE_DESCRIPTION`

---

## ğŸ¤– GitHub Actions Automation

This template includes `.github/workflows/scrape.yml`:

- `â°` Runs daily at `09:00 UTC`
- `ğŸ–±ï¸` Supports manual runs (`workflow_dispatch`)
- `ğŸ”` Retries scraper execution before failure (`SCRAPER_RUN_ATTEMPTS`, default `2`)
- `ğŸ“` Commits changed generated output/cache files
- `ğŸš¨` Optionally opens or updates a GitHub issue on failure (`CREATE_FAILURE_ISSUE=true`)

Recommended secrets:

- `CREATE_FAILURE_ISSUE` (`true`/`false`)
- `SCRAPER_RUN_ATTEMPTS` (integer)

---

## ğŸ—‚ï¸ Project Structure

- `scraper.py`: Main orchestration and output generation.
- `config.py`: Template configuration and defaults.
- `init_scraper.py`: Interactive bootstrap helper.
- `extractors/`: Source-specific extraction adapters.
- `html_templates.py`: Static HTML rendering templates.
- `utils.py`: Shared utilities.
- `.github/workflows/scrape.yml`: Automation example.

---

## ğŸ§© Dependencies

| Package | Purpose |
|---|---|
| `requests` | HTTP requests for source data |
| `beautifulsoup4` (optional) | HTML parsing extractor |
| `python-dotenv` (optional) | Environment variable loading from `.env` |

---

## ğŸ› ï¸ Troubleshooting

- `ğŸ”` If no events are generated, verify extraction method and selectors/path config.
- `ğŸ§±` Replace all placeholders in `config.py` before production use.
- `ğŸ“œ` For CI failures, inspect workflow logs and raise `SCRAPER_RUN_ATTEMPTS` if needed.

---

## ğŸ“„ License

[GPL-3.0](LICENSE)
